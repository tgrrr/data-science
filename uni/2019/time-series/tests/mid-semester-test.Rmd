---
title: "Time Series Cheatsheet R mid-semester exam"
author: "Phil Steinke s3725547"
subtitle: 'language: R mid-semester exam'
---

```{r presets}
rm(list = ls())
cat("\014") # clear everything

workingDirectory <- "/Users/phil/code/data-science/cheatsheet/source/includes/time-series/raw-data"

dataFilename <- "datasetMidSemTest.csv"
startYear = 1919 # ~old_todo~ get start year
# endYear = 0 # ~old_todo~ get start year
default_ylab = ''
# default_subtitle = ''
default_xlab = 'Year'
```

#### ~old_todo~ Hypothesis

<!-- ~old_todo~ -->
H<sub>0</sub>: the stochastic component of model 2 is normally distributed
H<sub>a</sub>: 

H<sub>0</sub>: μΔ=0
H<sub>a</sub>: μΔ≠0

We found that ... 

as the p-value was less than 5% and the null mean (i.e. zero) did not fall within the 95% confidence interval. As such the null hypothesis was rejected.

```{r Setup, message=FALSE, warning=FALSE, include=FALSE, warnings=0}
# library(forecast)
# library(fUnitRoots)
library('kableExtra')
library('knitr')
library('magrittr')
library('TSA') 
library(tseries) 
setwd(workingDirectory)
data <- read.csv(
  dataFilename, 
  header = F) 
# OR data <- read.table("data.csv", quote="\"", comment.char="")
```

```{r}
data %>% head(n = 5) # ~old_todo~ check for header, set startYear
data %>% tail(n = 5) # ~old_todo~ set endYear
```

```{r libraries, message=FALSE, warning=FALSE, include=FALSE, warnings=0}
# Add a ternary function in R: TRUE ? 'true' : 'false'
  `?` <- function(ifTernary, thenTernary)
      eval(
        sapply(
          strsplit(
            deparse(substitute(thenTernary)),
            ":"
        ),
        function(e) parse(text = e)
        )[[2 - as.logical(ifTernary)]])
# Setup kable to be used with 
# {r table_format, results = "hide", table = T, eval = F}
  default_source_hook <- knit_hooks$get('source')
  knit_hooks$set(
    source = function(x, options) {
      if(is.null(options$table))
        default_source_hook(x, options)
      else {
        eval(parse(text = x)) %>%
          kable("html") %>%
            kable_styling("hover", full_width = F)
      }  
    }
  )
```

```{r}
# ~old_todo~ check if it needs labelling
rownames(data) <- seq(from = startYear, to = endYear)

isTimeSeries <- function(data) { return(data %>% class() == 'ts') }

convertToTimeSeries <- function(data, startYear) {
  isTimeSeries(data) ?
    data.ts <- data :
    data.ts <- ts(
      as.vector(data), 
      start = startYear # ~old_fixme~ start_year
      # end = endYear # optional
    );
}
data.ts <- convertToTimeSeries(data, startYear)
cat('converted to time series data: ', isTimeSeries(data.ts))
```

```{r table_format, results = "hide", table = T, eval = F}
data.ts %>% dim()
data.ts %>% tail(n = 5)
```

### About the data

- 
- 
- 

```{r checkCorrelationWithPreviousYear}
checkCorrelationWithPreviousYear <- function(data) {
  y = data              # Read data into y
  x = zlag(data)        # Generate first lag 
  index = 2:length(x)   # Create an index to remove the first NA value and the last 5 missing values in x
  cor(y[index], x[index])
}
checkCorrelationWithPreviousYear(data.ts)
```
~old_todo~ There is a strong positive auto-correlation of the data for concurrent years

### Data Visualisation

- Seasonality
- Intonation point
- Change in variance
- Behaviour of the series (AR or MA)

```{r fig.cap='~old_todo~ caption'}
plot(
  data.ts
)
# OR ~old_later~ ~old_fixme~
# doTimeSeriesPlot(

```

### Fit the model

```{r}
# Define Linear Trends Model
t = time(data.ts) # Create time points for model fitting
linearTrendModel = lm(data.ts~t) # label the model
```

### Residuals
```{r}
summary(linearTrendModel)
```

- a p-value < 0.05 shows that the model is significant
- R<sup>2</sup> = 0.6655 shows the model is partially significant

Slope: β̂<sub>1</sub>= ~old_todo~
Intercept: β̂̂<sub>0</sb>0= ~old_todo~


#### f() ACF and PACF
```{r}
do.ACF.and.PACF <- function(data) {
  par(mfrow=c(1,2))
  acf(data) #Trend is apparent from ACF and PACf plots
  pacf(data)
  par(mfrow=c(1,1))
}
do.ACF.and.PACF(data.ts)
# Slowly decaying pattern in ACF and very high first correlation in PACF
# implies the existence of 
# - trend and 
# - non-stationarity.
```

#### ADF Test
```{r}
adf.test(data.ts)
```

#### Overcome the non-stationary nature of this series by using suitable tools
```{r}
options(warn=-1)
dataPositive <- data.ts + 10
dataPositive
# Let's first apply the Box-Cox transformation.
data.ts.transform = BoxCox.ar(
  dataPositive, 
  lambda = seq(-1,1.5,0.1),
  # method = "Nelder-Mead",
  hessian = T
  )
data.ts.transform$ci
lambda = 1 # The mid point of the interval ~old_todo~
BC.data.ts = (data.ts^lambda-1)/lambda # apply the Box-Cox transformation
```

```{r}
BC.data.ts
par(mfrow=c(1,1))
qqnorm(BC.data.ts)
qqline(BC.data.ts, col = 2)
```

```{r}
shapiro.test(BC.data.ts)
# In the QQ plot the tails of the distribution is far from the normality.
# The p-value of Shapiro test is less than 0.05; hence, we have enough 
# evidence to reject the normality hypothesis. In conclusion, the Box-Cox
# transformation did not significantly help to improve normality of the observations.
```

```{r}
plot(BC.data.ts,type='o',ylab='~old_todo~', main='Box-Cox ~old_todo~ ')
# In the time series plot, we observe that the variation in the series is 
# decreased after applying the Box-Cox transformation. But there is still trend.
# So let's apply the first difference and see if it helps.
```

```{r}
diff.BC.data.ts = diff(BC.data.ts)
par(mfrow=c(1,1))
plot(diff.BC.data.ts,type='o',ylab='~old_todo~')
# Now, there is only changing variance in the series after taking the first difference.
# Let's go on with the specification of the models.
```

```{r}
adf.test(diff.BC.data.ts)
# The null hypothesis of non-staionarity is rejected with a p-value of 0.01; hence,
# we conclude that the first differencing make the series stationary.
```

#### Specify the orders of `ARIMA(p,d,q)` model by using `ACF and PACF` plots
```{r}
par(mfrow=c(1,2))
acf(diff.BC.data.ts)
pacf(diff.BC.data.ts)
par(mfrow=c(1,1))
# There is a slowly decaying jagged pattern in ACF and 4 significant lags in the PACF.
# So we can consider ARI(4,1) model and those smaller than this model like ARI(1,1)
# ARI(2,1) and so on.
```

#### EACF plot
```{r}
eacf(diff.BC.data.ts)
# Although ther is no clear vertex in EACF, we can take the row corresponding to 
# p = 3 as the vertex and include ARIMA(3,1,2), ARIMA(3,1,3) and ARIMA(4,1,2)
# model into the set of possible models.

```

#### BIC table
```{r}
# ~old_todo~ (Hint: use "  nar=10, nma=10, ar.method='ols'   " options while displaying BIC table).
nar <- 10
nma <- 10
par(mfrow=c(1,1))
res2 = 
  armasubsets(
    y=diff.BC.data.ts,
    nar=nar,
    nma=nma,
    y.name='~old_todo~',
    ar.method='ols')
plot(res2)
# In the BIC table, shaded columns correspond to AR(4), MA(3) and MA(11) coefficients and 
# So, from here we can include ARIMA(4,1,2) and ARIMA(4,1,11) models in the set of 
# candidate model.

```


