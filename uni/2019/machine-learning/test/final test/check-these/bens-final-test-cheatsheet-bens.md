
# Shannonâ€™s Model of Entropy: 
[Drawing]Information Gain, entropy wrt target feature: 

```math
H(t,ğ’Ÿ)=âˆ’âˆ‘l âˆˆ levels(t)(P(t=l)Ã—log2(P(t=l)))
```

P(t=i)
 is probability that random element t is type i, l is number of different types of t, and s is arbitrary but usually 2 for calculating bits. 
#### Information Gain, entropy after partitioning: 

```math
rem(d,ğ’Ÿ)=âˆ‘lâˆˆlevels(d)âˆ£âˆ£ğ’Ÿd=lâˆ£âˆ£âˆ£âˆ£ğ’Ÿâˆ£âˆ£ Ã—H(t,ğ’Ÿd=l)
``` 

#### Information Gain: 
```math
IG(d,ğ’Ÿ)=H(t,ğ’Ÿ)âˆ’rem(d,ğ’Ÿ)

#### Information Gain Ratio: 

```math
GR(d,ğ’Ÿ)=IG(d,ğ’Ÿ)âˆ’âˆ‘lâˆˆlevels(d)(P(dâˆ’l)Ã—log2(P(d=l)))
```
Gini
```math
Gini(t,ğ’Ÿ)=1âˆ’âˆ‘lâˆˆlevels(t)P(t=l)2
```

```math
var(t,ğ’Ÿ)=âˆ‘ni=1(tiâˆ’tâˆ’ )2nâˆ’1
```
Euclidean
```math
Euclidean=âˆ‘i=1m(ğš[i]âˆ’ğ›[i])2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾î€â·î€€î€€
```
Man
```math
Manhattan=âˆ‘i=1mabs(ğš[i]âˆ’ğ›[i])2
```
```math
Minkowski(a,b)=(âˆ‘i=1mabs(ğš[i]âˆ’ğ›[i])2)1p
```
```math
simRR(ğª,ğ)=CP(ğª,ğ)âˆ£âˆ£ğªâˆ£âˆ£
```
```math
simSM(ğª,ğ)=CP(ğª,ğ)+CA(ğª,ğ)âˆ£âˆ£ğªâˆ£âˆ£
```
```math
simJ(ğª,ğ)=CP(ğª,ğ)CP(ğª,ğ)+PA(ğª,ğ)+AP(ğª,ğ)
```
```math
simcosine(a,b)=âˆ‘mi=1(a[i]Ã—b[i])âˆ‘mi=1a[i]2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšÃ—âˆ‘mi=1b[i]2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš
```

Binomial probability 
```math
P(x)=(nx)pxqnâˆ’x=n!(nâˆ’x)!âˆ™x!pxqnâˆ’x
```
```math
(nx)=n!(nâˆ’x)!âˆ™x!
```

n = num trials, x = num successes desired, p = prob success on one trial, q = 1-p 

Similarity metric criteria: 
Non-negativity, 
metric(ğš,ğ›)â‰¥0
```
```math 
Identity, 
metric(ğš,ğ›)=0â‡”ğš=ğ›
```
```math
Symmetry, 
metric(ğš,ğ›)=metric(ğ›,ğš)
```
```math
Triangular inequality: 
metric(ğš,ğ›)â‰¤metric(ğš,ğœ)+metric(ğ›,ğœ)
```
```math
K nearest neighbours: 
ğ•„k(q)=argmaxlâˆˆlevels(t)âˆ‘i=1kÎ´(ti,l)
```
```math
Weighted k nearest neighbour model: 
ğ•„k(q)=argmaxlâˆˆlevels(t)âˆ‘i=1k1dist(ğª,ğğ¢)2Î´(ti,l)
```
```math
ğ•„k(q)=1kâˆ‘ki=1ti (continuous target)
```
```math
[Drawing]Range normalisation: 
aâ€²i=aiâˆ’min(a)max(a)âˆ’min(a)Ã—(highâˆ’low)+low
```
```math
P(t=l âˆ£âˆ£ q[1],â€¦,1[m])=P(q[1],â€¦,q[m] âˆ£âˆ£ t=l)âˆ™P(t=l)P(q[1],â€¦,q[m)
```
```math
P(t=l)
 Prior probability 
```
```math
P(q[1],...,q[m])
 joint prob of descriptive features 
```
```math
P(q[1],â€¦,1[m] âˆ£âˆ£ t=l)
 conditional prob 
```
<!-- ```math
ğ•„(q)=argmaxlğœ–levels(t)P(t=lâˆ£âˆ£q[1],â€¦,q[m])
``` -->
<!-- ```math
ğ•„(q)=argmaxlğœ–levels(t)P([q]1,â€¦,q[m] âˆ£âˆ£ t=l)Ã—P(t=l)P(q[1],â€¦,q[m])
``` -->



| >      | >             | >    | >    | >                  | Prediction         |
| ------ | ------------- | ---- | ---- | ------------------ |:------------------:|
| Target | &nbsp;        | Pos+ | Neg- | >                  |     **Recall**     |
| ^      | Pos +         | TP   | FN   | $\frac{TP}{TP+FN}$ | $\frac{FN}{TP+FN}$ |
| ^      | Neg -         | FP   | TN   | $\frac{TN}{TN+FP}$ | $\frac{FP}{TN+FP}$ |
| ^      | **Precision** | >    | >    | $\frac{TP}{TP+FP}$ |                    |

precision=TP/(TP+FP)         
recall=TP/(TP+FN)

F<sub>1</sub> = $2Ã—\frac{precisionÃ—recall}{precision+recall}$

average class accuracy=1/âˆ£levels(t)âˆ£âˆ£âˆ‘lÏµlevels(t)recalll
 
average class accuracy<sub>HM</sub> = 11âˆ£âˆ£levels(t)âˆ£âˆ£âˆ‘lÏµlevels(t)1recalll

Stabâ€²ty Index=âˆ‘lÏµlevels(t)((âˆ£âˆ£ğ’œt=lâˆ£âˆ£âˆ£âˆ£ğ’œâˆ£âˆ£âˆ’âˆ£âˆ£â„¬t=lâˆ£âˆ£âˆ£âˆ£â„¬âˆ£âˆ£)Ã—ln(âˆ£âˆ£ğ’œt=lâˆ£âˆ£âˆ£âˆ£ğ’œâˆ£âˆ£âˆ£âˆ£â„¬t=lâˆ£âˆ£âˆ£âˆ£â„¬âˆ£âˆ£/))
 
Bayes=P(Xâˆ£âˆ£Y)=P(Yâˆ£âˆ£X)P(X)P(Y)=P(dâˆ£âˆ£t)=P(tâˆ£âˆ£d)P(d)P(t)

```math
Generalised Bayes Theorem: 
P(t=lâˆ£âˆ£q[1],â€¦,q[m])=P(q[1],â€¦,q[m]âˆ£âˆ£t=l)P(t=l)p(q[1],â€¦,q[m])
```
```math
Chain Rule: 
P(q[1],â€¦,q[m])=P(q[1])Ã—P(q[2]âˆ£âˆ£q[1])Ã—â€¦Ã—P(q[m]âˆ£âˆ£q[mâˆ’1],â€¦,q[2],q[1])
```
```math
NaÃ¯ve Bayesâ€™ Classifier: 
ğ•„(q)=argmaxlÏµlevels(t)(âˆmi=1P(q[i]âˆ£âˆ£t=l))Ã—P(t=l)
```
```math
Laplacian Smoothing (conditional probabilities): 
P(f=vâˆ£âˆ£t)=count(f=vâˆ£âˆ£t)+kcount(fâˆ£âˆ£t)+(kÃ—âˆ£âˆ£Domain(f)âˆ£âˆ£)
```
```math
ROC Index=âˆ‘âˆ£âˆ£Tâˆ£âˆ£i=2(FPR(T[i])âˆ’FPR(T[iâˆ’1])) Ã—((TPR(T[i])+TPR(T[iâˆ’1]))2
```
```math
Sum of squared errors=12âˆ‘ni=1(tiâˆ’ğ•„(di))2
```
```math
Mean squared error=âˆ‘ni=1(tiâˆ’ğ•„(di))2n
```
```math
Root MSE = MSEâ€¾â€¾â€¾â€¾â€¾âˆš
```
```math
R2=1âˆ’sum of squared errorstotal sum of squares
```
```math
logx(y)=ln(y)ln(x)          log2(y)=ln(y)ln(2)
```
